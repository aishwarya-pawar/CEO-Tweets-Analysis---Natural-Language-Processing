{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# numpy, pandas, matplotlib and regular expressions (data science essentials)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# styling\n",
    "pd.set_option('display.max_columns',150)\n",
    "plt.style.use('bmh')\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tweets\n",
    "df = pd.read_csv(\"ceo_tweets_final.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data colume to data type\n",
    "df[\"date\"] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for tweets from year 2016\n",
    "df = df[df['date'].dt.year>2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data colume to data type\n",
    "df[\"date\"]=df[\"date\"].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sahana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing stop words, punctuation and tokenizing\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + ['rt','amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        result = re.findall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)\", tweet) #(@[A-Za-z0-9]+)|\n",
    "        return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        result = re.findall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9_]+)\", tweet) #(@[A-Za-z0-9]+)|\n",
    "        return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_split(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        p = ' '.join(re.sub(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        s = ' '.join(re.sub(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9_]+)\", \" \", p).split())\n",
    "        return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", s).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lemmatization\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return lemma.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    \"\"\"\n",
    "    Get Bigrams\n",
    "    \"\"\"\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \"\"\"\n",
    "    Get Bigram Model, Corpus, id2word mapping\n",
    "    \"\"\"\n",
    "    bigram = bigrams(df.tweet_tokens_lem)\n",
    "    bigram = [bigram[tweet] for tweet in df.tweet_tokens_lem]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create separate colums for mentions, tags, tokens and process the tokens\n",
    "df[\"mentions\"] = df[\"tweet\"].apply(lambda tweet: get_mentions(tweet))\n",
    "df[\"tags\"] = df[\"tweet\"].apply(lambda tweet: get_hashtags(tweet))\n",
    "\n",
    "df[\"tweet_clean\"] = df[\"tweet\"].apply(lambda tweet: clean_tweet_split(tweet))\n",
    "\n",
    "df[\"tweet_tokens\"] = df[\"tweet_clean\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
    "df[\"tweet_tokens\"] = df[\"tweet_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])\n",
    "\n",
    "df[\"tweet_tokens_lem\"] = df[\"tweet_tokens\"].apply(lambda list_of_words: [lemmatize(x) for x in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Less Frequent words\n",
    "df[\"tweet_new\"] = df[\"tweet\"].apply(lambda tweet: remove_links(tweet))\n",
    "df[\"tweet_new\"] = df[\"tweet_new\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
    "df[\"tweet_new\"] = df[\"tweet_new\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tags</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_tokens_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>1200060640469159939</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>On this #Thanksgiving, I am reflecting on the ...</td>\n",
       "      <td>546</td>\n",
       "      <td>4434</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Thanksgiving, Dreamers]</td>\n",
       "      <td>On this , I am reflecting on the . As we enjoy...</td>\n",
       "      <td>[reflecting, enjoy, day, friends, family, feel...</td>\n",
       "      <td>[reflecting, enjoy, day, friend, family, feel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>1200047686180835328</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>Wishing everyone a #HappyThanksgiving filled w...</td>\n",
       "      <td>575</td>\n",
       "      <td>6097</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HappyThanksgiving]</td>\n",
       "      <td>Wishing everyone a filled with joy &amp;amp; happi...</td>\n",
       "      <td>[wishing, everyone, filled, joy, amp, happines...</td>\n",
       "      <td>[wishing, everyone, filled, joy, amp, happines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>1199872990718169089</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>Tomorrow the incredible @MNightShyamalan’s ser...</td>\n",
       "      <td>412</td>\n",
       "      <td>2607</td>\n",
       "      <td>[MNightShyamalan, Servant]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tomorrow the incredible ’s series premieres on...</td>\n",
       "      <td>[tomorrow, incredible, series, premieres, appl...</td>\n",
       "      <td>[tomorrow, incredible, series, premiere, apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>1199855397617704970</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>Thanksgiving Day challenge! Close your rings w...</td>\n",
       "      <td>394</td>\n",
       "      <td>3932</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Thanksgiving Day challenge! Close your rings w...</td>\n",
       "      <td>[thanksgiving, day, challenge, close, rings, f...</td>\n",
       "      <td>[thanksgiving, day, challenge, close, ring, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>1199767313890922497</td>\n",
       "      <td>2019-11-27</td>\n",
       "      <td>As many of you travel to be with loved ones to...</td>\n",
       "      <td>1310</td>\n",
       "      <td>8436</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>As many of you travel to be with loved ones to...</td>\n",
       "      <td>[many, travel, loved, ones, today, remember, p...</td>\n",
       "      <td>[many, travel, loved, one, today, remember, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    username                   id        date  \\\n",
       "0  @tim_cook  1200060640469159939  2019-11-28   \n",
       "1  @tim_cook  1200047686180835328  2019-11-28   \n",
       "2  @tim_cook  1199872990718169089  2019-11-28   \n",
       "3  @tim_cook  1199855397617704970  2019-11-28   \n",
       "4  @tim_cook  1199767313890922497  2019-11-27   \n",
       "\n",
       "                                               tweet  retweets  likes  \\\n",
       "0  On this #Thanksgiving, I am reflecting on the ...       546   4434   \n",
       "1  Wishing everyone a #HappyThanksgiving filled w...       575   6097   \n",
       "2  Tomorrow the incredible @MNightShyamalan’s ser...       412   2607   \n",
       "3  Thanksgiving Day challenge! Close your rings w...       394   3932   \n",
       "4  As many of you travel to be with loved ones to...      1310   8436   \n",
       "\n",
       "                     mentions                      tags  \\\n",
       "0                          []  [Thanksgiving, Dreamers]   \n",
       "1                          []       [HappyThanksgiving]   \n",
       "2  [MNightShyamalan, Servant]                        []   \n",
       "3                          []                        []   \n",
       "4                          []                        []   \n",
       "\n",
       "                                         tweet_clean  \\\n",
       "0  On this , I am reflecting on the . As we enjoy...   \n",
       "1  Wishing everyone a filled with joy &amp; happi...   \n",
       "2  Tomorrow the incredible ’s series premieres on...   \n",
       "3  Thanksgiving Day challenge! Close your rings w...   \n",
       "4  As many of you travel to be with loved ones to...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [reflecting, enjoy, day, friends, family, feel...   \n",
       "1  [wishing, everyone, filled, joy, amp, happines...   \n",
       "2  [tomorrow, incredible, series, premieres, appl...   \n",
       "3  [thanksgiving, day, challenge, close, rings, f...   \n",
       "4  [many, travel, loved, ones, today, remember, p...   \n",
       "\n",
       "                                    tweet_tokens_lem  \n",
       "0  [reflecting, enjoy, day, friend, family, feel,...  \n",
       "1  [wishing, everyone, filled, joy, amp, happines...  \n",
       "2  [tomorrow, incredible, series, premiere, apple...  \n",
       "3  [thanksgiving, day, challenge, close, ring, fa...  \n",
       "4  [many, travel, loved, one, today, remember, pr...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(df, username, num_topics):\n",
    "    \"\"\"\n",
    "    function for LDA analysis\n",
    "    \"\"\"\n",
    "    df_ceo = df[df['username']== username]\n",
    "    \n",
    "    all_words = df_ceo['tweet_new'].sum()\n",
    "    freq_dist = nltk.FreqDist(all_words)\n",
    "    df_fdist=pd.DataFrame(list(freq_dist.items()), columns=['term', 'freq'])\n",
    "    \n",
    "    df_fdist = df_fdist.sort_values(by = 'freq', ascending = False)\n",
    "    df_fdist = df_fdist[df_fdist['freq'] > 1]\n",
    "    \n",
    "    relevant_words = list(df_fdist['term'])\n",
    "    \n",
    "    df_ceo[\"tweet_new\"] = df_ceo[\"tweet_new\"].apply(lambda list_of_words: [x for x in list_of_words if x in relevant_words])\n",
    "    df_ceo[\"tweet_tokens_lem\"] = df_ceo[\"tweet_new\"].apply(lambda list_of_words: [lemmatize(x) for x in list_of_words])\n",
    "    \n",
    "    train_corpus, train_id2word, bigram_train = get_corpus(df_ceo)\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                               corpus=train_corpus,\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=train_id2word,\n",
    "                               chunksize=100,\n",
    "                               workers=7, # Num. Processing Cores - 1\n",
    "                               passes=50,\n",
    "                               eval_every = 1,\n",
    "                               per_word_topics=True,\n",
    "                               random_state=11)\n",
    "        lda_train.save('lda_train.model')\n",
    "        \n",
    "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
    "    \n",
    "    train_vecs = []\n",
    "    for i in range(len(df_ceo.tweet_new)):\n",
    "        top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(num_topics)]\n",
    "        train_vecs.append(topic_vec)\n",
    "    \n",
    "    return df_ceo, lda_train.print_topics(), train_vecs, num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_max_topics(values):\n",
    "#     topics = []\n",
    "#     if max(values) > 0.5:\n",
    "#         topics.append(max(values))\n",
    "#     elif len(list(set(values))) == 1:\n",
    "#         topics = values\n",
    "#     else:\n",
    "#         topics = [num for num in values if num > 0.5/(num_topics-1)]   \n",
    "    \n",
    "#     return topics\n",
    "\n",
    "## TRIED USING THE FUNCTION ABOVE. IT GAVE SIMILAR RESULTS AS THE BELOW FUNCTION\n",
    "\n",
    "def get_max_topics(values):\n",
    "    topics = []\n",
    "    if len(list(set(values))) == 1:\n",
    "        topics = values     \n",
    "    else:\n",
    "        topics.append(max(values))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def assign_topics(col1, col2):\n",
    "    if col1 in col2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tim Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.3803925329617725\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@tim_cook', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.103*\"year\" + 0.076*\"see\" + 0.072*\"developer\" + 0.058*\"world\" + 0.054*\"time\" + 0.045*\"week\" + 0.038*\"ago\" + 0.036*\"app\" + 0.035*\"powerful\" + 0.034*\"today\"'),\n",
       " (1,\n",
       "  '0.156*\"heart\" + 0.085*\"community\" + 0.079*\"family\" + 0.078*\"one\" + 0.046*\"today\" + 0.046*\"victim\" + 0.046*\"affected\" + 0.043*\"pro\" + 0.042*\"violence\" + 0.041*\"ipad\"'),\n",
       " (2,\n",
       "  '0.180*\"u\" + 0.060*\"make\" + 0.059*\"every\" + 0.053*\"celebrate\" + 0.050*\"life\" + 0.049*\"congratulation\" + 0.044*\"let\" + 0.040*\"day\" + 0.037*\"today\" + 0.036*\"people\"'),\n",
       " (3,\n",
       "  '0.170*\"thank\" + 0.148*\"work\" + 0.116*\"proud\" + 0.090*\"team\" + 0.072*\"great\" + 0.050*\"friend\" + 0.041*\"visit\" + 0.038*\"back\" + 0.037*\"th\" + 0.034*\"help\"'),\n",
       " (4,\n",
       "  '0.103*\"woman\" + 0.080*\"apple\" + 0.075*\"never\" + 0.065*\"story\" + 0.064*\"country\" + 0.062*\"right\" + 0.058*\"men\" + 0.058*\"enjoy\" + 0.057*\"like\" + 0.054*\"place\"'),\n",
       " (5,\n",
       "  '0.208*\"thanks\" + 0.123*\"iphone\" + 0.102*\"new\" + 0.076*\"love\" + 0.058*\"student\" + 0.047*\"forward\" + 0.043*\"thing\" + 0.041*\"shotoniphone\" + 0.033*\"customer\" + 0.031*\"photo\"'),\n",
       " (6,\n",
       "  '0.228*\"apple\" + 0.095*\"today\" + 0.057*\"thrilled\" + 0.055*\"store\" + 0.055*\"new\" + 0.052*\"great\" + 0.051*\"many\" + 0.045*\"future\" + 0.041*\"meet\" + 0.036*\"thanks\"'),\n",
       " (7,\n",
       "  '0.136*\"apple\" + 0.120*\"everyone\" + 0.102*\"around_world\" + 0.074*\"celebrating\" + 0.064*\"happy\" + 0.056*\"day\" + 0.047*\"thank\" + 0.044*\"wishing\" + 0.041*\"customer\" + 0.040*\"first\"')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Technology','Social','People','Appreciation','Women','Product','Store Launch','Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tim_cook = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tim_cook['all_topics']= df_tim_cook[['Technology','Social','People','Appreciation','Women','Product','Store Launch','Emotion']].values.tolist()\n",
    "df_tim_cook['max_topics'] = df_tim_cook['all_topics'].apply(lambda values: get_max_topics(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tim_cook['Technology'] = df_tim_cook.apply(lambda x: assign_topics(x['Technology'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Social'] = df_tim_cook.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_tim_cook['People'] = df_tim_cook.apply(lambda x: assign_topics(x['People'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Appreciation'] = df_tim_cook.apply(lambda x: assign_topics(x['Appreciation'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Women'] = df_tim_cook.apply(lambda x: assign_topics(x['Women'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Product'] = df_tim_cook.apply(lambda x: assign_topics(x['Product'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Store Launch'] = df_tim_cook.apply(lambda x: assign_topics(x['Store Launch'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Emotion'] = df_tim_cook.apply(lambda x: assign_topics(x['Emotion'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_tim_cook[['Technology','Social','People','Appreciation','Women','Product','Store Launch','Emotion']].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "py.plotly.tools.set_credentials_file(username='sah_lumos', api_key='9fCFTwIksEv3WNQFIZSL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21b2d6bf898>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot pie chart for the major topics with their contribution in total number of tweets\n",
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "colors = Spectral_8.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Tim Cook')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = df_tim_cook['tags'].apply(pd.Series).stack()\n",
    "\n",
    "hashtags_df= pd.DataFrame(hashtags)\n",
    "hashtags_df.columns=['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df = pd.DataFrame(hashtags_df['hashtags'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df['index'] = hashtags_df['index'].apply(lambda x:\"Holiday Celebration\" if any(y in x.lower() for y in [\"easter\",\"day\",\"diwali\",\"july\",\"month\",\"year\",\"thanksgiving\",\"week\"]) else x)\n",
    "hashtags_df['index'] = hashtags_df['index'].apply(lambda x:\"Apple\" if any(y in x.lower() for y in [\"apple\",\"airpod\",\"iphone\",\"ipad\",\"potrait\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df = pd.DataFrame(hashtags_df[\"index\"].value_counts()).reset_index()\n",
    "hashtag_df.columns= [\"hashtags\",\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.613861386138616 % of Tim Cook's hashtags are about general wishes on holidays\n"
     ]
    }
   ],
   "source": [
    "print(((hashtag_df[hashtag_df[\"hashtags\"] == 'Holiday Celebration'][\"count\"]/hashtag_df[\"count\"].sum())*100).values[0],\"% of Tim Cook's hashtags are about general wishes on holidays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.881188118811881 % of Tim Cook's hashtags are about apple products\n"
     ]
    }
   ],
   "source": [
    "print(((hashtag_df[hashtag_df[\"hashtags\"] == 'Apple'][\"count\"]/hashtag_df[\"count\"].sum())*100).values[0],\"% of Tim Cook's hashtags are about apple products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bill Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.39186093801872235\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@BillGates', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.150*\"one\" + 0.072*\"year\" + 0.071*\"book\" + 0.044*\"lot\" + 0.027*\"recently\" + 0.027*\"favorite\" + 0.027*\"read\" + 0.023*\"last\" + 0.021*\"never\" + 0.020*\"optimistic\"'),\n",
       " (1,\n",
       "  '0.061*\"need\" + 0.046*\"vaccine\" + 0.040*\"alzheimer\" + 0.039*\"excited\" + 0.030*\"look\" + 0.028*\"government\" + 0.028*\"new\" + 0.028*\"ever\" + 0.028*\"disease\" + 0.027*\"world\"'),\n",
       " (2,\n",
       "  '0.058*\"life\" + 0.054*\"great\" + 0.045*\"work\" + 0.044*\"melinda\" + 0.037*\"student\" + 0.034*\"best\" + 0.034*\"new\" + 0.033*\"could\" + 0.033*\"aid\" + 0.031*\"learn\"'),\n",
       " (3,\n",
       "  '0.049*\"like\" + 0.040*\"melindagates\" + 0.035*\"warrenbuffett\" + 0.035*\"future\" + 0.034*\"time\" + 0.032*\"know\" + 0.032*\"story\" + 0.032*\"think\" + 0.029*\"new\" + 0.028*\"hope\"'),\n",
       " (4,\n",
       "  '0.092*\"world\" + 0.068*\"progress\" + 0.056*\"health\" + 0.036*\"see\" + 0.035*\"u\" + 0.033*\"global\" + 0.032*\"making\" + 0.031*\"incredible\" + 0.026*\"always\" + 0.022*\"child\"'),\n",
       " (5,\n",
       "  '0.063*\"people\" + 0.047*\"world\" + 0.046*\"energy\" + 0.045*\"today\" + 0.044*\"help\" + 0.035*\"get\" + 0.033*\"thing\" + 0.027*\"poverty\" + 0.026*\"young\" + 0.025*\"make\"')]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bill_gates = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_bill_gates['all_topics']= df_bill_gates[['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']].values.tolist()\n",
    "df_bill_gates['max_topics'] = df_bill_gates['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_bill_gates['Book Recommendations'] = df_bill_gates.apply(lambda x: assign_topics(x['Book Recommendations'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Diseases/Vaccines'] = df_bill_gates.apply(lambda x: assign_topics(x['Diseases/Vaccines'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Education'] = df_bill_gates.apply(lambda x: assign_topics(x['Education'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Warren Buffet'] = df_bill_gates.apply(lambda x: assign_topics(x['Warren Buffet'], x['max_topics']), axis=1)\n",
    "df_bill_gates['General World Issues'] = df_bill_gates.apply(lambda x: assign_topics(x['General World Issues'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Renewable Energy'] = df_bill_gates.apply(lambda x: assign_topics(x['Renewable Energy'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_bill_gates[['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21b24ff0d30>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "colors = Spectral_8.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Bill Gates')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = df_bill_gates['tags'].apply(pd.Series).stack()\n",
    "\n",
    "hashtags_df= pd.DataFrame(hashtags)\n",
    "hashtags_df.columns=['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Goalkeepers18</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MosquitoWeek</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomentofLift</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alzheimers</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataviz</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWD2019</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malaria</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorldMalariaDay</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBT</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndPolio</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endpolio</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RedNoseDay</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLKDay</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremereading</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWD2018</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wef19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchforAfrica</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USOpen</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StepUpTheFight</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IndianElections2019</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GearVR</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OnePlanetSummit</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Principles</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThankATeacher</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigDebate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WD2019</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BookLoversDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UKAid</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndTB</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endmalaria</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorldTBDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClimateAction</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IAmHartmut</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wimbledon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyNextGuest</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWD</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEF18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DACA</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThanksPaul</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goalkeepers19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GivingTuesday</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyGivingStory</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorldToiletDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HitRefresh</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMA</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIDSReport</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EqualityCantWait</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorldAIDSDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EarthDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BattleoftheSexes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BestFriendsDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorldMosquitoDay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sxsw</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mandela100</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goalkeepers17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perovskites</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHC17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WithoutMom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hashtags\n",
       "Goalkeepers18               8\n",
       "MosquitoWeek                4\n",
       "MomentofLift                4\n",
       "VR                          4\n",
       "Alzheimers                  3\n",
       "dataviz                     3\n",
       "IWD2019                     2\n",
       "malaria                     2\n",
       "WorldMalariaDay             2\n",
       "TBT                         2\n",
       "EndPolio                    2\n",
       "endpolio                    2\n",
       "RedNoseDay                  2\n",
       "MLKDay                      2\n",
       "extremereading              2\n",
       "IWD2018                     1\n",
       "wef19                       1\n",
       "MatchforAfrica              1\n",
       "USOpen                      1\n",
       "StepUpTheFight              1\n",
       "IndianElections2019         1\n",
       "GearVR                      1\n",
       "OnePlanetSummit             1\n",
       "Principles                  1\n",
       "ThankATeacher               1\n",
       "BigDebate                   1\n",
       "WD2019                      1\n",
       "BookLoversDay               1\n",
       "UKAid                       1\n",
       "EndTB                       1\n",
       "...                       ...\n",
       "endmalaria                  1\n",
       "WorldTBDay                  1\n",
       "ClimateAction               1\n",
       "IAmHartmut                  1\n",
       "Wimbledon                   1\n",
       "MyNextGuest                 1\n",
       "IWD                         1\n",
       "WEF18                       1\n",
       "DACA                        1\n",
       "ThanksPaul                  1\n",
       "Goalkeepers19               1\n",
       "GivingTuesday               1\n",
       "MyGivingStory               1\n",
       "WorldToiletDay              1\n",
       "HitRefresh                  1\n",
       "AMA                         1\n",
       "AIDSReport                  1\n",
       "EqualityCantWait            1\n",
       "WorldAIDSDay                1\n",
       "EarthDay                    1\n",
       "BattleoftheSexes            1\n",
       "BestFriendsDay              1\n",
       "WorldMosquitoDay            1\n",
       "sxsw                        1\n",
       "Mandela100                  1\n",
       "Goalkeepers17               1\n",
       "perovskites                 1\n",
       "AI                          1\n",
       "GHC17                       1\n",
       "WithoutMom                  1\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hashtags_df['hashtags'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar analysis was done for all other CEOs to understand what kind of topics they talk aboout. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
